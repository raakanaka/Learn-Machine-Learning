{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1e2ROcq1WX1SEMBmKl-71GXkqq2LbPiXK",
      "authorship_tag": "ABX9TyNqML2SIWNXkQPSKo10SgkA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafiqmyura/Learn-Machine-Learning/blob/main/Disable_Sign_People_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_cv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sm05Pt0wb7K",
        "outputId": "9955137d-1943-4c55-cd78-f5c715c1aabc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: namex, keras-core, keras_cv\n",
            "Successfully installed keras-core-0.1.7 keras_cv-0.8.1 namex-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xy53yWCN-KY",
        "outputId": "e296cb04-9986-4980-e4d0-d7f0ca431821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import keras_cv\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "GLOBAL_CLIPNORM = 10.0\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Preprocessing with TensorFlow"
      ],
      "metadata": {
        "id": "_CXO7IWXnkU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2H2GIROLnkTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_txt_annot(img_path, txt_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    w = int(img.shape[0])\n",
        "    h = int(img.shape[1])\n",
        "\n",
        "    file_label = open(txt_path, \"r\")\n",
        "    lines = file_label.read().split('\\n')\n",
        "\n",
        "    boxes = []\n",
        "    classes = []\n",
        "\n",
        "    for i in range(0, int(len(lines))):\n",
        "        objbud=lines[i].split(' ')\n",
        "        class_ = int(objbud[0])\n",
        "\n",
        "        x1 = float(objbud[1])\n",
        "        y1 = float(objbud[2])\n",
        "        w1 = float(objbud[3])\n",
        "        h1 = float(objbud[4])\n",
        "\n",
        "        xmin = int((x1*w) - (w1*w)/2.0)\n",
        "        ymin = int((y1*h) - (h1*h)/2.0)\n",
        "        xmax = int((x1*w) + (w1*w)/2.0)\n",
        "        ymax = int((y1*h) + (h1*h)/2.0)\n",
        "\n",
        "        boxes.append([xmin ,ymin ,xmax ,ymax])\n",
        "        classes.append(class_)\n",
        "\n",
        "    return img_path, classes, boxes\n",
        "\n",
        "\n",
        "# a function for creating file paths list\n",
        "def create_paths_list(path):\n",
        "    full_path = []\n",
        "    images = sorted(os.listdir(path))\n",
        "\n",
        "    for i in images:\n",
        "        full_path.append(os.path.join(path, i))\n",
        "\n",
        "    return full_path\n",
        "\n",
        "\n",
        "class_ids = ['apart_parking', 'blackbox', 'disabled_parking', 'etc', 'highpass', 'navigator']\n",
        "class_mapping = dict(zip(range(len(class_ids)), class_ids))"
      ],
      "metadata": {
        "id": "7ZhKcPg8oCjM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def creating_files(img_files_paths, annot_files_paths):\n",
        "\n",
        "    img_files = create_paths_list(img_files_paths)\n",
        "    annot_files = create_paths_list(annot_files_paths)\n",
        "\n",
        "    image_paths = []\n",
        "    bbox = []\n",
        "    classes = []\n",
        "\n",
        "    for i in range(0,len(img_files)):\n",
        "        image_path_, classes_, bbox_ = parse_txt_annot(img_files[i], annot_files[i])\n",
        "        image_paths.append(image_path_)\n",
        "        bbox.append(bbox_)\n",
        "        classes.append(classes_)\n",
        "\n",
        "    image_paths = tf.ragged.constant(image_paths)\n",
        "    bbox = tf.ragged.constant(bbox)\n",
        "    classes = tf.ragged.constant(classes)\n",
        "\n",
        "    return image_paths, classes, bbox"
      ],
      "metadata": {
        "id": "0N9WaK4wrI5r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_paths, train_classes, train_bboxes = creating_files ('/content/drive/MyDrive/Data Disable Detection/train/images', '/content/drive/MyDrive/Data Disable Detection/train/labels')\n",
        "\n",
        "valid_img_paths, valid_classes, valid_bboxes = creating_files ('/content/drive/MyDrive/Data Disable Detection/valid/images', '/content/drive/MyDrive/Data Disable Detection/valid/labels')\n",
        "\n",
        "test_image_paths, test_classes, test_bboxes = creating_files ('/content/drive/MyDrive/Data Disable Detection/test/images', '/content/drive/MyDrive/Data Disable Detection/test/labels')"
      ],
      "metadata": {
        "id": "SMbNXM90pwkf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Dataset"
      ],
      "metadata": {
        "id": "ZBVu6dOJrOvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def img_preprocessing(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels = 3)\n",
        "    img = tf.cast(img, tf.float32)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "resizing = keras_cv.layers.JitteredResize(\n",
        "    target_size=(640, 640),\n",
        "    scale_factor=(0.75, 1.3),\n",
        "    bounding_box_format=\"xyxy\")\n",
        "\n",
        "# loading dataset\n",
        "def load_ds(img_paths, classes, bbox):\n",
        "    img = img_preprocessing(img_paths)\n",
        "\n",
        "    bounding_boxes = {\n",
        "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
        "        \"boxes\": bbox }\n",
        "\n",
        "    return {\"images\": img, \"bounding_boxes\": bounding_boxes}\n",
        "\n",
        "def dict_to_tuple(inputs):\n",
        "    return inputs[\"images\"], inputs[\"bounding_boxes\"]"
      ],
      "metadata": {
        "id": "_RCtcZS8rOcB"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}